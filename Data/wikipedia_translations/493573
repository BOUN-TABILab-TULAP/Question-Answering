K-en yakın komşu algoritması
Örüntü tanıma, "k" -Nearest Komşular algoritması (ya da "k" kısa -NN) sınıflandırma ve regresyon için kullanılan parametrik olmayan bir yöntemdir. Her iki durumda da, giriş özelliği alanı "k" en yakın eğitim örnekleri oluşur. Çıktı "k" -NN sınıflandırması ya da gerileme için kullanılan bağlıdır:
"K" -NN fonksiyonu sadece yerel yaklaşılır örnek tabanlı öğrenme, ya da tembel öğrenme türüdür ve tüm hesaplama sınıflandırması kadar ertelenmiştir. "K" -NN algoritma tüm makine öğrenme algoritmaları basit arasındadır.
Hem sınıflandırma ve regresyon için, bu yakın komşular daha uzak olanlara göre ortalama daha fazla katkıda böylece komşular katkılarını ağırlık yararlı olabilir. Örneğin, ortak bir ağırlıklandırma şeması "D" komşu mesafe 1 / "d", her Komşuna bir ağırlık vererek oluşur.
Komşular hangi sınıf veya nesne özelliği değeri ("k" -NN sınıflandırma) bilinmektedir (için -NN regresyon "k") nesneler kümesinden alınır. Açık bir eğitim aşaması gereklidir ama bu, algoritma eğitim seti olarak düşünülebilir.
"K" -NN algoritmasının bir eksiklik o verilerin yerel yapısına duyarlı olmasıdır.
Algoritma.
eğitim örnekleri çok boyutlu bir özellik uzayda vektörler, bir sınıf etiketine sahip her biri. algoritmanın eğitim aşaması sadece eğitim örneklerinin özellik vektörlerini ve sınıf etiketleri saklamak oluşur.
Sınıflandırma aşamasında, "k" olan bir kullanıcı tanımlı sabit ve bir etiketsiz vektör (sorgu veya test noktası) yakın bu sorgu noktasına "k" eğitim örnekleri arasında en sık etiketi atayarak sınıflandırılır.
Sürekli değişkenler için metrik Yaygın olarak kullanılan bir mesafe Öklid mesafedir. Metin sınıflandırma gibi farklı değişkenler, başka bir metrik bu üst üste binme metrik (veya Hamming uzaklık) olarak kullanılabilir. Gen ekspresyonu mikro-dizi verileri bağlamında, örneğin, "k" -NN ayrıca Pearson ve Spearman Korelasyon katsayıları ile kullanılmıştır. Mesafe metrik tür Geniş Teminat Yakın Komşu veya Mahalle bileşenler analizi gibi özel algoritmalar ile öğrenmiş ise sık sık, "k" -NN sınıflandırılması doğruluğu önemli ölçüde geliştirilmiş olabilir.
Sınıf dağılımı çarpık olduğunda temel "çoğunluk oylama" sınıflandırma bir dezavantajı ortaya çıkar. Yani nedeniyle çok sayıda "k" yakın komşuları arasında ortak olma eğilimindedir, çünkü daha sık sınıfın örnekleri, yeni örnek tahminini hakim eğilimi vardır. Bu sorunun üstesinden gelmenin bir yolu olarak "k" yakın komşularının her dikkate test noktası mesafe alarak, sınıflandırma kilo olduğunu. "k" yakın noktalarının her birinin (regresyon problemlerinde veya değer) sınıf test noktasına bu noktadan mesafe ters orantılı bir ağırlık ile çarpılır. Çarpık üstesinden gelmek için başka bir yolu veri gösterimi soyutlama gereğidir. Kendi kendini örgütleyen bir harita (SOM) Örneğin, her düğüm ne olursa olsun orijinal eğitim verileri kendi yoğunluğu, benzer noktaları bir küme temsilcisi (merkez) 'dir. "K" -NN daha sonra SOM'daki uygulanabilir.
Parametre seçimi.
"k" en iyi seçim verilerine bağlıdır; Genellikle, "k" büyük değerler sınıflandırılmasına ilişkin gürültü etkisini azaltmak, ancak sınıflar arasındaki sınırların daha az belirgin hale. İyi bir "k" çeşitli sezgisel teknikler (hyperparameter optimizasyon bakınız) seçilebilir. Sınıf tahmin ediliyor özel durum yakın eğitim örneğinin sınıfı olmak üzere ("k" = 1, yani zaman) en yakın komşu algoritması denir.
"k" -NN algoritmanın doğruluğu ciddi gürültülü veya alakasız özelliklerin varlığı ile bozulmuş olabilir, ya da özellik ölçekleri önemlerine uygun değilse. Çok araştırma çabası seçerek veya sınıflandırma artırmak için özellikler ölçekleme girmiştir. Özellikle popüler bir yaklaşım özelliği ölçekleme optimize etmek için evrimsel algoritmaların kullanılmasıdır. Bir başka popüler bir yaklaşım eğitim sınıfları ile eğitim verilerinin karşılıklı bilgi ile özellikleri ölçek etmektir.
İkili (iki sınıfı) sınıflandırma problemleri, bu "k" seçmek için bu bağladı oy önler gibi bir tek sayı olması yararlıdır. Bu ortamda ampirik optimal "k" seçme Bir popüler yolu önyükleme yöntemi ile olduğunu.
Özellikleri.
"K" -NN değişken bant genişliği düzgün bir çekirdek ile, çekirdek yoğunluğu "balon" tahmincisi özel bir durumdur.
algoritmanın naif sürümü tüm saklanan örnekler test örnek mesafeleri hesaplayarak uygulanması kolay, ama büyük eğitim setleri için hesaplama yoğun olduğunu. Uygun bir en yakın komşu arama algoritmasını kullanarak yapar "k-" Hatta büyük veri kümeleri için hesaplama uysal NN. Birçok yakın komşu arama algoritmaları yılda ileri sürülmüştür; bunlar genellikle mesafe değerlendirmeler sayısını fiilen gerçekleştirilen azaltmak istiyoruz.
"K-" NN bazı güçlü tutarlılık sonuçları vardır. Veri miktarı sonsuz yaklaşırken, algoritma iki Bayes hata oranı daha kötü bir hata oranı (veri dağılımı verilen asgari ulaşılabilir hata oranı) elde garantilidir. "K" -NN "k" (veri noktalarının sayısının bir fonksiyonu olarak nerede "k" artar) bazı değer Bayes hata oranını yaklaşmak için garanti edilir. "K" -NN Çeşitli iyileştirmeler yakınlık grafikler kullanarak mümkündür.
Metrik Öğrenme.
K-en yakın komşu sınıflandırma performansı genellikle belirgin (denetimli) metrik öğrenme yoluyla geliştirilebilir. Popüler algoritmalar Mahalle bileşenler analizi ve Büyük marjı en yakın komşu bulunmaktadır. Denetimli metrik öğrenme algoritmaları, yeni bir metrik veya pseudo-metrik öğrenmek için etiket bilgileri kullanın.
Özellik çıkarma.
Bir algoritmaya giriş verileri çok büyük olduğunda işlenecek ve herkesin bildiği gereksiz olduğundan şüphelenilen (örneğin her iki ayak ve metre olarak aynı ölçüm) sonra giriş veri özellikleri (adlandırılmış özellikleri azaltılmış temsil seti dönüşecek vektör). Özellikler kümesi içine girdi verilerini Dönüşümü özellik çıkarma denir. Ayıklanır özellikler özenle seçilmiş ise bu özellikler kümesi, bu azalma temsil yerine tam boy girişini kullanarak istenen görevi gerçekleştirmek için veri girişi alakalı bilgileri ayıklamak beklenmektedir. Özellik çıkarma öncesinde özellik uzayda dönüştürülmüş verileri "k" -NN algoritma uygulanarak ham verilere yapılır.
(Genellikle OpenCV ile uygulanan) özelliği çıkarma ve boyut küçültme ön işleme adımları içeren "k" -NN kullanarak yüz tanıma için tipik bir bilgisayar vizyonu hesaplama boru hattının bir örnek:
Boyut küçültme.
Boyut indirgeme (10 daha fazla boyutlara sayısı ile örneğin), yüksek boyutlu veriler için genellikle boyutluluk laneti etkilerini önlemek amacıyla "k" -NN algoritma uygulanmadan önce gerçekleştirilir.
"k" in boyutluluğun -NN bağlamında laneti temelde tüm vektörler arama sorgusu vektörü neredeyse eşit olduğundan Öklid uzaklığı (birden noktaları sorgu noktasında olan bir daire daha az ya da yalan hayal yüksek boyutlarda yararsızdır demektir merkezi; arama uzayda tüm veri noktalarına sorgudan mesafe) hemen hemen aynıdır.
Özellik çıkarma ve boyut küçültme "k" ile kümelenme ardından temel bileşen analizi (PCA), lineer diskriminant analizi (LDA), ya da bir ön işleme adımı olarak kanonik korelasyon analizi (CCA) teknikler kullanılarak tek bir adımda kombine edilebilir -NN indirgenmiş boyutlu uzayda nitelik vektörleri üzerinde. Makine öğrenimi, bu sürecin aynı zamanda düşük boyutlu katıştırma denir.
Çok yüksek boyutlu veri setleri (örneğin canlı video akışları, DNA veri veya yüksek boyutlu zaman serisi bir benzerlik arama yaparken) hızlı yaklaşık "k" -NN arama yerellik duyarlı karma kullanarak çalışan, "rastgele projeksiyonlar", "skeçler için "ya da VLDB araç kutusundan diğer yüksek boyutlu benzerlik arama teknikleri uygulanabilir tek seçenek olabilir.
Karar sınır.
Yürürlükte yakın komşu kuralları örtülü karar sınırını hesaplamak. Açıkça karar sınırını hesaplamak için, ve hesaplama karmaşıklığı sınır karmaşıklığı bir fonksiyonu olduğunu, böylece çok verimli yapmak da mümkündür.
Veri azaltma.
Veri indirgeme büyük veri setleri ile çalışmak için en önemli sorunlardan biridir. Genellikle, sadece veri noktalarının bazıları, doğru sınıflandırma için ihtiyaç vardır. Aşağıdaki gibi Bu veriler "prototipler" olarak adlandırılır ve bulunabilir:
Sınıf aykırı seçimi.
Diğer sınıfların örnekleri ile çevrili bir eğitim örneği sınıf outlier denir. Sınıf aykırı nedenleri şunlardır:
"K" ile Sınıf aykırı -NN gürültü üretirler. Bunlar tespit edilir ve daha sonraki analiz için ayrılabilir. Onun "k" yakın komşuları diğer sınıfların "r" örnekler daha eklerseniz Verilen iki doğal sayılar "k> r> 0", bir eğitim örneği "(k, r)" NN sınıf Aykırı denir.
Veri azaltılması için CNN.
Yoğun yakın komşu (CNN, "Hart algoritması") için ayarladığınız veriyi azaltmak üzere tasarlanmış bir algoritma -NN sınıflandırma "k". Eğitim setinde, gelen prototiplerin kümesi "U" seçer öyle ki 1NN ile 1NN tüm veri seti ile yaptığı gibi "U" neredeyse kadar doğru örnekler sınıflandırabiliriz.
Bir eğitim seti "X" göz önüne alındığında, CNN iteratif çalışır:
Sınıflandırma için yerine "X" "U" kullanın. prototipler olmayan örnekler "absorbe" puan olarak adlandırılır.
Bu sınır oranının azalan eğitim örneklerini taramak için etkilidir. bir eğitim Örneğin "x", sınır oranı olarak tanımlanır
|| "xy" || yakın örnek mesafe "y" "x" farklı bir renge sahip ve || "X'-y" olduğu || "en yakın örnek" y "mesafe olduğunu x 'x' 'ile aynı etiketle ".
Sınır oranı aralığı [0,1] Çünkü || "X'-y" || asla aşıyor || "xy" || bulunmaktadır. Bu sıralama prototip set "U" eklenmesi için sınıfların sınırları tercih verir. "X" 'den daha farklı bir etiketin bir nokta, "x" harici olarak adlandırılır. Çerçeve oranının hesaplanması Sağdaki ile görüntülenmiştir. veri noktaları renklerle etiketlenmiştir: Başlangıç ​​noktası "X" ve onun etiketi kırmızıdır. Dış noktaları mavi ve yeşil. En yakın dış noktası "y" ise "x". yakın "y" kırmızı nokta "x" "dir. Sınır oranı "a" ("x") = || "X'-y" || / || "xy" || başlangıç ​​noktası "x" nin niteliğidir.
Aşağıdaki şekillerde, bir dizi CNN bir gösterimidir. (Kırmızı, yeşil ve mavi) üç sınıf vardır. İncir. 1: Başlangıçta her sınıfta 60 puan var. İncir. 2 1NN sınıflandırma haritasını göstermektedir: Her piksel tüm verileri kullanılarak 1NN tarafından sınıflandırılır. İncir. 3 5NN sınıflandırması haritasını gösterir. (Yeşil ikisini 5 en yakın komşular arasında iki kırmızı ve bir mavi noktalar varsa, örneğin) Beyaz alanlar 5NN oylama bağlıdır sınıflandırılamayan bölgelere karşılık gelir. İncir. 4 azaltılmış veri kümesi gösterir. haçlar (3,2) NN kuralı (bu örnekleri her üç yakın komşuları diğer sınıflara mensup) tarafından seçilen sınıf aykırı olduğu; kareler prototipler vardır ve boş çevreler absorbe noktalarıdır. Sol alt köşede sınıfı-aykırı, prototip numaralarını gösterir ve her üç sınıf için puan absorbe. Prototip sayısı, bu örnekte farklı sınıflar için% 20% 15 arasında değişir. İncir. 5 prototip ile 1NN sınıflandırma haritası ilk veri seti ile çok benzer olduğunu göstermektedir. rakamlar Mirkes uygulamasını kullanarak üretildi.
"K" -NN regresyon.
"K" -NN regresyonda, "k" -NN algoritma sürekli değişkenler tahmin etmek için kullanılır. Böyle bir algoritma kendi mesafenin tersi ile ağırlıklandırılmış "k" yakın komşuları ağırlıklı ortalama kullanır. Bu, aşağıdaki gibi algoritma çalışır:
Sonuçların Doğrulama.
Bir karışıklık matris veya "eşleştirme matrix" sık sık "k" -NN sınıflandırma doğruluğunu doğrulamak için bir araç olarak kullanılır. Bu tür olabilirlik oranı testi gibi daha sağlam istatistiki yöntemler de uygulanabilir.
